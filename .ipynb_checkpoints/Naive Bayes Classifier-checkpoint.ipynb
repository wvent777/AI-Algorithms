{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load\n",
    "`load` takes in a file name and then returns a numpy array of the file **Used by**: None\n",
    "\n",
    "* **file_name** str: the file name\n",
    "\n",
    "**returns** list[list] : returns a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file_name: str) -> list[list]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e' 'x' 's' ... 'n' 'c' 'l']\n",
      " ['p' 'f' 'y' ... 'h' 'y' 'p']\n",
      " ['e' 'f' 's' ... 'u' 'v' 'd']\n",
      " ...\n",
      " ['p' 'k' 'y' ... 'w' 'v' 'p']\n",
      " ['e' 'x' 'y' ... 'n' 'y' 'd']\n",
      " ['p' 'f' 's' ... 'h' 'v' 'u']]\n"
     ]
    }
   ],
   "source": [
    "print(load('agaricus-lepiota.data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_domain\n",
    "`get_domain` examines the data at a column index to return all the unique values within that column **Used by**: [cross_validation](#cross_validation), [train](#train)\n",
    "* **data** list[list]: the data\n",
    "* **col** int: the index of the column\n",
    "\n",
    "**returns** list: returns a list containing the unique valeus in that col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(data:list[list], col:int)->list:\n",
    "    return np.unique(data[:, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [['s', 's', 'a'], \n",
    "        ['s', 's', 's'], \n",
    "        ['s', 'a', 'b']]\n",
    "test = np.asarray(test1)\n",
    "\n",
    "assert len(get_domain(test, 0)) == 1\n",
    "assert len(get_domain(test, 1)) == 2\n",
    "assert len(get_domain(test, 2)) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_count\n",
    "`get_count` takes in the data, column index, value string character, and if necessary the y column index and y value chracter string to get the count of said character, if a y column and y character is inputted then it'll return the logical count of instances where both characters are present **Used by**: [get_probability](#get_probability)\n",
    "\n",
    "* **data** list[list]: the data \n",
    "* **col** int: the index of the column\n",
    "* **value** str: the character of the string in question that is being counted\n",
    "* **y_col** int: the index of the class - y column \n",
    "* **y_value** str: the character of the class from the y_column that is in question \n",
    "\n",
    "**returns** int: returns the count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(data: list[list], col:int, value:str, y_col = None, y_value = None)->int:\n",
    "    value_count = (data[:, col] == value)\n",
    "    if y_col is None or y_value is None:\n",
    "        return value_count.sum()\n",
    "    y_count = (data[:, y_col] == y_value)\n",
    "    val_y_count = np.logical_and(value_count, y_count)\n",
    "    return val_y_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [['s', 's', 'a'], \n",
    "        ['s', 's', 's'], \n",
    "        ['s', 'a', 'b']]\n",
    "test = np.asarray(test1)\n",
    "\n",
    "assert get_count(test, 0, 's') == 3\n",
    "assert get_count(test, 1, 's') == 2\n",
    "assert get_count(test, 1, 's', y_col = 0, y_value = 's') == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_col_count\n",
    "`get_col_count` gets the count of the length of the columns **Used by:** [get_probability](#get_probability)\n",
    "\n",
    "* **data** list[list]: the data in question\n",
    "* **col** int: the index of the column\n",
    "\n",
    "**returns** int: returns the count of how many instances are in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_count(data:list[list], col:int)-> int:\n",
    "    return len(data[:, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [['s', 's', 'a'], \n",
    "        ['s', 's', 's'], \n",
    "        ['s', 'a', 'b']]\n",
    "test = np.asarray(test1)\n",
    "\n",
    "assert get_col_count(test, 0) == 3\n",
    "\n",
    "test2 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 's']])\n",
    "\n",
    "assert get_col_count(test2, 0) == 2\n",
    "\n",
    "test3 = np.asarray([['s', 's', 's']])\n",
    "\n",
    "assert get_col_count(test3, 0) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_probability \n",
    "`get_probability` gets the probability of a string character based on how many times it appears within the column, if y_values and y_col are not none then it'll return the conditional probability given the instance is also y_value. \n",
    "This automatically defaults to smoothig, if smoothing is False then it will not perform +1 smoothing **Used by:** [train](#train)\n",
    "* **data** list[list]: the data in question\n",
    "* **col** int: the index of the colum\n",
    "* **value** str: the character of the string in question that is being counted\n",
    "* **y_col** int: the index of the class - y column \n",
    "* **y_value** str: the character of the class from the y_column that is in question \n",
    "* **smoothing** bool: if True then performs +1 Smoothing if False then it doesn't do plus 1 smoothing\n",
    "\n",
    "**returns** float: returns the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(data: list[list], col: int, value:str, y_col = None, y_value = None, smoothing = True)-> float:\n",
    "    if y_col is None or y_value is None:\n",
    "        value_count = get_count(data, col, value)\n",
    "        col_count = get_col_count(data, col)\n",
    "        value_prob = value_count/col_count\n",
    "        return value_prob\n",
    "    else:\n",
    "        # Doing the probability of X given Y\n",
    "        value_count = get_count(data, col, value, y_col, y_value)\n",
    "        y_count = get_count(data, y_col, y_value)\n",
    "        if smoothing != True:\n",
    "            value_prob = value_count / y_count\n",
    "            return value_prob\n",
    "        value_prob = (value_count + 1) / (y_count +1)\n",
    "    return value_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a'], \n",
    "                    ['s', 'a', 'a'], \n",
    "                    ['s', 'a', 'a']])\n",
    "assert get_probability(test1, 1, 'a') == 0.5\n",
    "\n",
    "assert get_probability(test1, 0, 's') == 1\n",
    "\n",
    "assert get_probability(test1, 1, 'a', y_col = 0, y_value = 'a') == 1\n",
    "\n",
    "assert get_probability(test1, 1, 'a', y_col = 0, y_value = 's', smoothing = False) == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "\n",
    "`train` gets the naive bayes classifer as a list of dictionaries\n",
    "\n",
    "* **data** list[list]: the data in question\n",
    "* **smoothing** bool: if True then performs +1 smoothing is False then it does not\n",
    "\n",
    "**returns** list[dict]: returns all the possible probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data:list[list], smoothing=True) -> list[dict]:\n",
    "    nbc = []\n",
    "    domains = [get_domain(data, i) for i in range(len(data[0]))]\n",
    "    y_values = domains[0]\n",
    "    nbc.append({value: get_probability(data, 0, value) for value in domains[0]})\n",
    "    for i in range(1, len(data[0])):\n",
    "        nbc.append({value: {y_value : get_probability(data, i, value, 0, y_value, smoothing=smoothing) \n",
    "                                      for y_value in y_values} for value in domains[i]})\n",
    "    return nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a'], \n",
    "                    ['s', 'a', 'a'], \n",
    "                    ['s', 'a', 'a']])\n",
    "assert train(test1) == [{'s': 1.0}, {'a': {'s': 0.6}, 's': {'s': 0.6}}, {'a': {'s': 1.0}}]\n",
    "\n",
    "test2 = np.asarray([['s', 's', 'a'], \n",
    "                    ['e', 's', 'a'], \n",
    "                    ['s', 'a', 'a'], \n",
    "                    ['e', 'a', 'a']])\n",
    "assert train(test2) == [{'e': 0.5, 's': 0.5},\n",
    "                                        {'a': {'e': 0.6666666666666666, 's': 0.6666666666666666},\n",
    "                                         's': {'e': 0.6666666666666666, 's': 0.6666666666666666}},\n",
    "                                        {'a': {'e': 1.0, 's': 1.0}}]\n",
    "\n",
    "assert train(test2, smoothing = False) == [{'e': 0.5, 's': 0.5},\n",
    "                                                           {'a': {'e': 0.5, 's': 0.5}, 's': {'e': 0.5, 's': 0.5}},\n",
    "                                                           {'a': {'e': 1.0, 's': 1.0}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_prob_of\n",
    "`get_prob_of` calculates the probability of an example given a specific class label **Used by:** [normalize](#normalize),  [best](#best), [classify](#classify)\n",
    "\n",
    "$label = P(C) \\cdot \\prod_{i,j} P(A_i = V_j | C)$\n",
    "\n",
    " $C$ = class label\n",
    " $A$ = attribute\n",
    " $V$ = attribute value\n",
    " \n",
    "* **nbc** list[dict]: naive bayes data stucture\n",
    "* **instance** list: instance of attributes\n",
    "* **label** str: label in question \n",
    "\n",
    "**returns** float: the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_of(nbc: list[dict], instance:list, label:str)-> float:\n",
    "    return nbc[0][label]*np.product(np.array([nbc[i+1][instance[i]][label] \n",
    "                                                            for i in range(len(instance))]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a'], \n",
    "                    ['s', 'a', 'a'], \n",
    "                    ['s', 'a', 'a']])\n",
    "\n",
    "assert get_prob_of(train(test1), test1[1][1:], label=\"s\") == 0.6\n",
    "assert get_prob_of(train(test1, smoothing=False), test1[1][1:], label=\"s\") == 0.5\n",
    "\n",
    "test2 = np.asarray([['s', 's', 'a'], \n",
    "                    ['e', 's', 'a'], \n",
    "                    ['s', 'a', 'a'], \n",
    "                    ['e', 'a', 'a']])\n",
    "assert get_prob_of(train(test2, smoothing=False), test1[1][1:], label=\"e\") == 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize \n",
    "\n",
    "The normalize function will normalize the results so that the probabilities add up to 1 **Used by:**[best](#best), [classify](#classify)\n",
    "\n",
    "* **nbc** dict: modified dictionary structure of the nbc\n",
    "* **labels** list[str]: list of all the possible labels  \n",
    "\n",
    "**returns** dict: normalized probailities given the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(nbc: dict, labels:list[str])->dict:\n",
    "    total = np.sum(np.array([nbc[label] for label in labels]))\n",
    "    result = {label : nbc[label]/total for label in labels}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a'], \n",
    "                    ['e', 'a', 'a'], \n",
    "                    ['s', 'a', 'a']])\n",
    "test_labels = ['e', 's']\n",
    "probs = train(test1)\n",
    "\n",
    "results1 = {label : get_prob_of(probs, test1[2][1:], label)  for label in test_labels}\n",
    "results2 = {label : get_prob_of(probs, test1[1][1:], label)  for label in test_labels}\n",
    "results3 = {label : get_prob_of(probs, test1[3][1:], label)  for label in test_labels}\n",
    "\n",
    "assert normalize(results1, test_labels) == {'e': 0.4, 's': 0.6}\n",
    "assert normalize(results2, test_labels) == {'e': 0.18181818181818182, 's': 0.8181818181818182}\n",
    "assert normalize(results3, test_labels) == {'e': 0.4, 's': 0.6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best\n",
    "`best` takes the naive bayes data structure and the sorts it by the highest probability and returns the label that has the highest probabilty **Used by:** [classify](#classify)\n",
    "\n",
    "* **nbc** dict: modified dictionary structure of the nbc\n",
    "* **labels** list[str]: list of all the possible labels  \n",
    "\n",
    "**returns** str: returns the best choice within the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best(nbc:dict, labels:list[str])->str:\n",
    "    return sorted([(nbc[label], label) for label in labels], reverse = True)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a'], \n",
    "                    ['e', 'a', 'a'], \n",
    "                    ['s', 'a', 'a']])\n",
    "test_labels = ['e', 's']\n",
    "probs = train(test1)\n",
    "\n",
    "results1 = {label : get_prob_of(probs, test1[2][1:], label)  for label in test_labels}\n",
    "results2 = {label : get_prob_of(probs, test1[1][1:], label)  for label in test_labels}\n",
    "results3 = {label : get_prob_of(probs, test1[3][1:], label)  for label in test_labels}\n",
    "\n",
    "assert best(normalize(results1, test_labels), test_labels) == 's'\n",
    "assert best(normalize(results2, test_labels), test_labels) == 's'\n",
    "assert best(normalize(results3, test_labels), test_labels) == 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify\n",
    "`classify` is used to classify an instance from the testing data \n",
    "* **nbc** list[dict]: naive bayes data stucture\n",
    "* **instance** list[str]: instance of attributes\n",
    "* **labels** list[str]: list of possible labels\n",
    "\n",
    "**returns**  tuple : best choice and the dictionary of the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(nbc, instance, labels)-> tuple:\n",
    "    res = {label: get_prob_of(nbc, instance, label) for label in labels}\n",
    "    res = normalize(res, labels)\n",
    "    best_choice = best(res, labels)\n",
    "    return (best_choice, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a'], \n",
    "                    ['e', 'a', 'a'], \n",
    "                    ['s', 'a', 'a']])\n",
    "test_labels = ['e', 's']\n",
    "test_nbc = train(test1)\n",
    "\n",
    "\n",
    "assert classify(test_nbc, test1[2][1:], test_labels) == ('s', {'e': 0.4, 's': 0.6})\n",
    "\n",
    "assert classify(test_nbc, test1[1][1:], test_labels) == ('s', {'e': 0.18181818181818182, 's': 0.8181818181818182})\n",
    "\n",
    "assert classify(test_nbc, test1[3][1:], test_labels) == ('s', {'e': 0.4, 's': 0.6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_folds\n",
    "`get_fold` is used to split the data into k folds\n",
    "\n",
    "* **data** list[list]: the data in question\n",
    "* **k** int: the number of folds\n",
    "\n",
    "**returns**  list[list]: nested list with k number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(data:list[list], k:int)->list[list]:\n",
    "    return np.array_split(data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a'], \n",
    "                    ['e', 'a', 'a'], \n",
    "                    ['s', 'a', 'a']])\n",
    "\n",
    "assert len(get_folds(test1, 3)) == 3\n",
    "assert len(get_folds(test1, 4)) == 4\n",
    "assert len(get_folds(test1, 1)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate \n",
    "`separate` is used to separate the x and y values\n",
    "* **data** list[list]: the data in question\n",
    "\n",
    "**returns** list, list: list of x and list of y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(data):\n",
    "    x = data[:,1:]\n",
    "    y = data[:, 0]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a'], \n",
    "                    ['e', 'a', 'a'], \n",
    "                    ['s', 'a', 'a']])\n",
    "separate(test1)\n",
    "x = [['s', 'a'],\n",
    "     ['s', 'a'],\n",
    "     ['a', 'a'],\n",
    "     ['a', 'a']]\n",
    "y = ['s', 's', 'e', 's']\n",
    "assert separate(test1) == x or y\n",
    "\n",
    "test2 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'a']])\n",
    "x2 = [['s', 'a'],\n",
    "      ['s', 'a']]\n",
    "y2 = ['s', 's']\n",
    "assert separate(test2) == x2 or y2 \n",
    "\n",
    "test3 = np.asarray([['s', 's', 'a']])\n",
    "x3 = [['s', 'a']]\n",
    "y3 = ['s']\n",
    "assert separate(test2) == x3 or y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validation\n",
    "`cross_validation` is used to execute all of the above and classify the testing data in each fold for k number of folds\n",
    "\n",
    "* **data** list[list]: the data in question\n",
    "* **k** int: the number of folds\n",
    "* **smoothing** bool: +1 smoothing if True, if False then no smoothing\n",
    "\n",
    "**returns** list(tuples): list of tuples with y actual and the y predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data:list[list], k: int, smoothing=True)-> list:\n",
    "    outputs = []\n",
    "    np.random.shuffle(data)\n",
    "    folds = get_folds(data, k)\n",
    "    labels = get_domain(data, 0)\n",
    "    for i in range(k):\n",
    "        test_data = folds[i][:,:]\n",
    "        new_folds = np.row_stack(np.delete(folds, i, 0))\n",
    "        training_data = new_folds[:,:]\n",
    "        nbc = train(training_data, smoothing=smoothing)\n",
    "        test_x, y = separate(test_data)\n",
    "        y_pred = np.array([classify(nbc, test_x[i], labels) for i in range(len(test_x))])\n",
    "    \n",
    "        outputs.append((y, y_pred))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.asarray([['s', 's', 'a'], \n",
    "                    ['s', 's', 'b'], \n",
    "                    ['e', 'a', 'a'], \n",
    "                    ['s', 'a', 'a'], \n",
    "                    ['e', 's', 'b'], \n",
    "                    ['e', 'a', 'a'],\n",
    "                    ['e', 's', 'a'], \n",
    "                    ['s', 'a', 'b'], \n",
    "                    ['e', 's', 'a'], \n",
    "                    ['e', 'a', 'a']])\n",
    "\n",
    "output = cross_validation(test1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate\n",
    "`evaluate` is used to evalute the error rate by counting the number of times the y_pred does not equal the y_actual and dividing it by the total of predictions, and then it also evalutates the variance in error rate for all the folds\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output)->float:\n",
    "    fold_errors = []\n",
    "    total_error_count, total_size = 0, 0\n",
    "    for y, y_pred in output:\n",
    "        error_count = 0\n",
    "        total_size += len(y)\n",
    "        for a, b in zip(y, y_pred):\n",
    "            if a != b[0]:\n",
    "                error_count +=1\n",
    "                total_error_count +=1\n",
    "            else:continue\n",
    "        fold_errors.append(error_count/len(y))\n",
    "    total_error = total_error_count/total_size\n",
    "    variance = sum([(fold - mean(fold_errors))**2 for fold in fold_errors]) / (len(fold_errors)-1)\n",
    "    return fold_errors, total_error, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_error: [0.6, 0.6]\n",
      "total_error: 0.6\n",
      "variance: 0.0\n"
     ]
    }
   ],
   "source": [
    "a,b,c = evaluate(output)\n",
    "print(\"fold_error:\", a)\n",
    "print(\"total_error:\", b)\n",
    "print(\"variance:\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pprint\n",
    "`pprint` is a helper function to pretty print the outputs of running the cross_validation testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(output):\n",
    "    fold_error, total_error, variance = evaluate(output)\n",
    "    for i in range(len(fold_error)):\n",
    "        print(f\"Fold: {i+1}\")\n",
    "        print(f\"  error rate_(fold = {i+1}) = {fold_error[i]*100} %\\n\")\n",
    "    print(f\"Total error rate = {total_error*100} %\")\n",
    "    print(f\"Variance = {variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "  error rate_(fold = 1) = 60.0 %\n",
      "\n",
      "Fold: 2\n",
      "  error rate_(fold = 2) = 60.0 %\n",
      "\n",
      "Total error rate = 60.0 %\n",
      "Variance = 0.0\n"
     ]
    }
   ],
   "source": [
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Fold Cross Validation on Mushroom Data with +1 Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wsven\\anaconda3\\envs\\en605645\\lib\\site-packages\\numpy\\lib\\function_base.py:5030: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "  error rate_(fold = 1) = 4.305043050430505 %\n",
      "\n",
      "Fold: 2\n",
      "  error rate_(fold = 2) = 4.182041820418204 %\n",
      "\n",
      "Fold: 3\n",
      "  error rate_(fold = 3) = 3.6900369003690034 %\n",
      "\n",
      "Fold: 4\n",
      "  error rate_(fold = 4) = 4.428044280442804 %\n",
      "\n",
      "Fold: 5\n",
      "  error rate_(fold = 5) = 3.8177339901477834 %\n",
      "\n",
      "Fold: 6\n",
      "  error rate_(fold = 6) = 3.9408866995073892 %\n",
      "\n",
      "Fold: 7\n",
      "  error rate_(fold = 7) = 4.926108374384237 %\n",
      "\n",
      "Fold: 8\n",
      "  error rate_(fold = 8) = 4.310344827586207 %\n",
      "\n",
      "Fold: 9\n",
      "  error rate_(fold = 9) = 6.157635467980295 %\n",
      "\n",
      "Fold: 10\n",
      "  error rate_(fold = 10) = 5.41871921182266 %\n",
      "\n",
      "Total error rate = 4.517479074347612 %\n",
      "Variance = 5.9829302837869466e-05\n"
     ]
    }
   ],
   "source": [
    "mushroom_data = load('agaricus-lepiota.data')\n",
    "\n",
    "mushroom_output_smoothing = cross_validation(mushroom_data, 10)\n",
    "\n",
    "pprint(mushroom_output_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Fold Cross Validation on Mushroom Data with no smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "  error rate_(fold = 1) = 0.4920049200492005 %\n",
      "\n",
      "Fold: 2\n",
      "  error rate_(fold = 2) = 0.24600246002460024 %\n",
      "\n",
      "Fold: 3\n",
      "  error rate_(fold = 3) = 0.24600246002460024 %\n",
      "\n",
      "Fold: 4\n",
      "  error rate_(fold = 4) = 0.6150061500615006 %\n",
      "\n",
      "Fold: 5\n",
      "  error rate_(fold = 5) = 0.0 %\n",
      "\n",
      "Fold: 6\n",
      "  error rate_(fold = 6) = 0.7389162561576355 %\n",
      "\n",
      "Fold: 7\n",
      "  error rate_(fold = 7) = 0.12315270935960591 %\n",
      "\n",
      "Fold: 8\n",
      "  error rate_(fold = 8) = 0.49261083743842365 %\n",
      "\n",
      "Fold: 9\n",
      "  error rate_(fold = 9) = 0.49261083743842365 %\n",
      "\n",
      "Fold: 10\n",
      "  error rate_(fold = 10) = 0.0 %\n",
      "\n",
      "Total error rate = 0.34465780403741997 %\n",
      "Variance = 6.668084280250639e-06\n"
     ]
    }
   ],
   "source": [
    "mushroom_output_no_smoothing = cross_validation(mushroom_data, 10, smoothing=False)\n",
    "\n",
    "pprint(mushroom_output_no_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts:\n",
    "\n",
    "After running the 10 fold cross validations with +1 smoothing and without it, it is noticeable that the error rate without the +1 smoothing is less than the 10-fold cross validation with the +1 smoothing. Laplace smoothing is a technique that adds 1 to the count of all n-observations in the training data before normalizing the probabilities. This is done to eliminate the instance of zero-probability. While the error-rate is higher with Laplace smoothing, the error rates across all the the folds look much more uniformed than in the later. Without Laplace smoothing, the error rate has a lot if variance among each fold. Laplace smoothing ensures that the posterior probability is never zero, and helps make successful predictions when a query point contains a new observation which has not been seen in the training data, therfore by accounting for unseen observations the error rate is bound to rise. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
